# modules/evaluator.py
import streamlit as st
import pandas as pd
import json, re, os
from groq import Groq
from dotenv import load_dotenv
from langdetect import detect
from deep_translator import GoogleTranslator
from modules.extractors import extract_text_with_pages

# ØªØ­Ù…ÙŠÙ„ Ù…ÙØªØ§Ø­ Groq Ù…Ù† .env
load_dotenv()
client = Groq(api_key=os.getenv("GROQ_API_KEY"))

# ===========================================================
# ğŸ”¤ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
# ===========================================================
def translate_if_needed(criteria_list, text):
    """Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Øµ Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠÙ‹Ø§ ØªÙØªØ±Ø¬Ù… Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§"""
    try:
        sample = text[:1000]
        lang = detect(sample)
        if lang == "en":
            st.info("ğŸ”¤ ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø£Ù† Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŒ ÙŠØ¬Ø±ÙŠ ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±...")
            translated = [
                GoogleTranslator(source="ar", target="en").translate(c)
                for c in criteria_list
            ]
            return translated, "en"
    except Exception:
        pass
    return criteria_list, "ar"


# ===========================================================
# ğŸ§  Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ø¹Ø±ÙˆØ¶
# ===========================================================
@st.cache_data(show_spinner=False)
def evaluate_offers(offers, criteria_list):
    results, details = [], {}

    for f in offers:
        with st.spinner(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ø±Ø¶: {f.name}"):
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ
            data = extract_text_with_pages(f)
            if isinstance(data, dict):
                if data.get("type") == "pdf":
                    text = "\n".join(p["text"] for p in data.get("pages", []))
                elif data.get("type") == "docx":
                    text = data.get("text", "")
                else:
                    text = ""
            else:
                text = str(data)

            if not text.strip():
                st.warning(f"âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ ÙŠÙ…ÙƒÙ† ØªØ­Ù„ÙŠÙ„Ù‡ ÙÙŠ Ø§Ù„Ù…Ù„Ù: {f.name}")
                continue

            # ØªØ±Ø¬Ù…Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø¥Ø°Ø§ Ù„Ø²Ù…
            criteria_list, lang_detected = translate_if_needed(criteria_list, text)
            text_criteria = "\n".join([f"- {c}" for c in criteria_list])

            # ===== Ø§Ù„ØªÙˆØ¬ÙŠÙ‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ =====
            prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø¹Ø±ÙˆØ¶ ØªÙ‚Ù†ÙŠØ©. Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø«Ù… Ù‚ÙŠÙ‘Ù… Ø§Ù„Ø¹Ø±Ø¶ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.

Ù„ÙƒÙ„ Ù…Ø¹ÙŠØ§Ø±:
- Ø¶Ø¹ Ø¯Ø±Ø¬Ø© Ù…Ù† 1 Ø¥Ù„Ù‰ 4 (1=Ø¶Ø¹ÙŠÙØŒ 4=Ù…Ù…ØªØ§Ø²)
- Ø§ÙƒØªØ¨ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø°ÙŠ Ø·Ø±Ø­ØªÙ‡ Ù„ØªÙ‚ÙŠÙŠÙ…Ù‡ (ai_question)
- Ø§ÙƒØªØ¨ Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ (reason)

Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„:
{{
  "scores": [
    {{"criterion":"...","score":3,"ai_question":"...","reason":"..."}}
  ],
  "overall_comment": "Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø§Ù…Ø© Ø¹Ù† Ø§Ù„Ø¹Ø±Ø¶"
}}

Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±:
{text_criteria}

Ø§Ù„Ù†Øµ:
{text[:18000]}
"""

            try:
                response = client.chat.completions.create(
                    model="llama-3.3-70b-versatile",
                    temperature=0.3,
                    max_tokens=3500,
                    messages=[{"role": "user", "content": prompt}],
                )
                result_text = response.choices[0].message.content.strip()

                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON
                json_match = re.search(r"\{.*\}", result_text, re.S)
                if not json_match:
                    st.warning(f"âš ï¸ Ù„Ù… ÙŠÙØ±Ø¬Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ JSON ØµØ§Ù„Ø­ Ù„Ù„Ù…Ù„Ù: {f.name}")
                    continue

                data_json = json.loads(json_match.group(0))
                scores = data_json.get("scores", [])
                comment = data_json.get("overall_comment", "â€” Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø¹Ø§Ù…Ø© â€”")

                df = pd.DataFrame(scores)
                for col in ["criterion", "score", "reason", "ai_question"]:
                    if col not in df.columns:
                        df[col] = ""

                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØºØ±ÙŠØ¨Ø© (Ù…Ø«Ù„ Ø§Ù„ØµÙŠÙ†ÙŠØ©)
                for c in ["reason", "ai_question"]:
                    df[c] = df[c].astype(str).apply(lambda x: re.sub(r"[^\u0600-\u06FFa-zA-Z0-9\s.,()%-]", "", x))

                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø·
                df["score"] = pd.to_numeric(df["score"], errors="coerce").fillna(0)
                overall = df["score"].mean() / 4

                results.append({
                    "file": f.name,
                    "overall": overall,
                    "comment": comment
                })
                details[f.name] = df

            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ {f.name}: {e}")
                # Ø­ØªÙ‰ Ù„Ùˆ ÙØ´Ù„ Ø¹Ø±Ø¶ ÙˆØ§Ø­Ø¯ØŒ Ù†Ø­ÙØ¸ ØµÙ Ø§ÙØªØ±Ø§Ø¶ÙŠ
                results.append({
                    "file": f.name,
                    "overall": 0.0,
                    "comment": f"Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}"
                })
                details[f.name] = pd.DataFrame()

    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ DataFrame
    if results:
        ranked = pd.DataFrame(results)
        if "overall" not in ranked.columns:
            ranked["overall"] = 0.0
        ranked = ranked.sort_values("overall", ascending=False).reset_index(drop=True)
        return ranked, details
    else:
        st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø£ÙŠ Ø¹Ø±ÙˆØ¶.")
        return pd.DataFrame(columns=["file", "overall", "comment"]), {}

# modules/analyzer.py
import os, json, hashlib, re
import streamlit as st
from groq import Groq
import fitz
import pytesseract
from PIL import Image

# ============================================================
# â˜ï¸ Ø¥Ø¹Ø¯Ø§Ø¯ Groq (Ø³Ø­Ø§Ø¨ÙŠ ÙÙ‚Ø·)
# ============================================================
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
client = Groq(api_key=GROQ_API_KEY) if GROQ_API_KEY else None

def _md5(s: str) -> str:
    return hashlib.md5(s.encode("utf-8", "ignore")).hexdigest()

# ============================================================
# ğŸ§  Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø©
# ============================================================
def _safe_json_loads(s: str):
    """ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ JSON Ø­ØªÙ‰ Ù„Ùˆ Ø£Ø¶Ø§Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù†ØµÙˆØµØ§Ù‹ Ø²Ø§Ø¦Ø¯Ø©."""
    try:
        return json.loads(s)
    except Exception:
        m = re.search(r"(\[.*\])", s, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(1))
            except:
                pass
    return None


def clean_text(text):
    """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØºØ±ÙŠØ¨Ø© ÙˆØ§Ù„ÙØ±Ø§ØºØ§Øª."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s\.\,\-\(\)\/%]', '', text)
    return text.strip()


def extract_text_with_ocr(pdf_bytes, show_progress=True):
    """
    ğŸ§  Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ø¯Ù‚ÙŠÙ‚ Ù…Ù† PDF:
    - ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ø¥Ù† ÙˆÙØ¬Ø¯
    - ÙŠÙØ¹Ù‘Ù„ OCR Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø©
    - ÙŠØ¹Ø±Ø¶ Ø´Ø±ÙŠØ· ØªÙ‚Ø¯Ù… ÙÙŠ Streamlit
    """
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    total = len(doc)
    ocr_count = 0

    progress_bar = st.progress(0)
    for i, page in enumerate(doc):
        text = page.get_text("text").strip()
        used_ocr = False

        if len(text) < 40:  # Ø¥Ø°Ø§ Ø§Ù„ØµÙØ­Ø© ÙÙ‚ÙŠØ±Ø© Ù†ØµÙŠÙ‹Ø§ â†’ Ø§Ø³ØªØ®Ø¯Ù… OCR
            pix = page.get_pixmap(dpi=200)
            img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
            text = pytesseract.image_to_string(img, lang="ara+eng")
            used_ocr = True
            ocr_count += 1

        text = clean_text(text)
        pages.append({"page_num": i + 1, "text": text, "ocr_used": used_ocr})

        # ØªØ­Ø¯ÙŠØ« Ø´Ø±ÙŠØ· Ø§Ù„ØªÙ‚Ø¯Ù…
        progress_bar.progress((i + 1) / total)

    if show_progress:
        percent_ocr = (ocr_count / total) * 100
        if ocr_count > 0:
            st.warning(f"ğŸŸ¨ ØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… OCR ÙÙŠ {ocr_count} ØµÙØ­Ø© ({percent_ocr:.1f}%).")
        else:
            st.success("ğŸŸ© ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙØ­Ø§Øª Ù†ØµÙŠÙ‹Ø§ Ø¨Ø¯ÙˆÙ† Ø§Ù„Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ OCR.")

    return {"type": "pdf", "pages": pages}


# ============================================================
# ğŸ§  Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Groq (Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬)
# ============================================================
def _llm_json_only(prompt: str, model=None) -> str:
    """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Groq ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© ÙƒÙ†Øµ ÙÙ‚Ø· (ÙŠØªÙˆÙ‚Ø¹ JSON)."""
    if not client:
        raise RuntimeError("âš ï¸ GROQ_API_KEY ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ·.")

    selected_model = model or "llama-3.3-70b-versatile"
    resp = client.chat.completions.create(
        model=selected_model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.25,
        max_tokens=4000
    )
    return resp.choices[0].message.content.strip()


# ============================================================
# ğŸ“„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ
# ============================================================
def analyze_sections_with_pages(doc_payload: dict):
    st.info("ğŸ¤– Ø¬Ø§Ø±Ù ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ Ø¨Ø¯Ù‚Ø© Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ÙƒØ§Ù…Ù„Ø©...")

    if doc_payload.get("type") == "pdf":
        parts = [f"[[PAGE:{p['page_num']}]]\n{p['text']}" for p in doc_payload["pages"]]
        full_text = "\n\n".join(parts)
        chunks = [full_text[i:i+18000] for i in range(0, len(full_text), 18000)]
        all_sections = []

        for idx, chunk in enumerate(chunks):
            st.caption(f"ğŸ“„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø²Ø¡ {idx+1}/{len(chunks)}...")
            prompt = f"""
Ø§Ù‚Ø±Ø£ Ø§Ù„Ù†Øµ Ø£Ø¯Ù†Ø§Ù‡ Ù…Ù† Ø¹Ø±Ø¶ ÙÙ†ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù„Ø§Ù…Ø§Øª ØµÙØ­Ø§Øª Ø¨Ø§Ù„Ø´ÙƒÙ„ [[PAGE:n]].
Ù‚Ø³Ù‘Ù…Ù‡ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ø«Ù„:
Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ø§Ù„Ø£Ù‡Ø¯Ø§ÙØŒ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©ØŒ Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ØŒ Ø§Ù„ÙØ±ÙŠÙ‚ØŒ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ù„Ø®Ø§ØªÙ…Ø©.

Ù„ÙƒÙ„ Ù‚Ø³Ù… Ø£Ø¹Ø¯ JSON Ø¨Ù‡Ø°Ø§ Ø§Ù„Ø´ÙƒÙ„ ÙÙ‚Ø·:
[
  {{
    "section": "Ø§Ø³Ù… Ø§Ù„Ù‚Ø³Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©",
    "start_page": Ø±Ù‚Ù… Ø§Ù„ØµÙØ­Ø©,
    "summary": "Ù…Ù„Ø®Øµ Ù‚ØµÙŠØ± ÙˆÙˆØ§Ø¶Ø­",
    "content": "Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù‚Ø³Ù…"
  }}
]

âš ï¸ Ù„Ø§ ØªØ¶Ù Ø£ÙŠ Ù†Øµ Ø®Ø§Ø±Ø¬ JSON.
Ø§Ù„Ù†Øµ:
{chunk}
"""
            try:
                reply = _llm_json_only(prompt)
                data = _safe_json_loads(reply)
                if data:
                    all_sections.extend(data)
            except Exception as e:
                st.error(f"âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¬Ø²Ø¡ {idx+1}: {e}")

        merged = {}
        for sec in all_sections:
            name = sec.get("section", "").strip()
            if not name:
                continue
            if name in merged:
                merged[name]["content"] += "\n" + sec.get("content", "")
                merged[name]["summary"] = merged[name]["summary"] or sec.get("summary", "")
                merged[name]["start_page"] = min(merged[name]["start_page"], sec.get("start_page", 1))
            else:
                merged[name] = sec

        out = sorted(merged.values(), key=lambda x: x["start_page"])
        return out

    elif doc_payload.get("type") == "docx":
        text = doc_payload["text"]
        prompt = f"""
Ù‚Ø³Ù‘Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ Ø£Ù‚Ø³Ø§Ù… ÙˆØ§Ø¶Ø­Ø© Ù…Ø«Ù„ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©ØŒ Ø§Ù„Ø£Ù‡Ø¯Ø§ÙØŒ Ø§Ù„Ù…Ù†Ù‡Ø¬ÙŠØ©ØŒ Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°ØŒ Ø§Ù„ÙØ±ÙŠÙ‚ØŒ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ Ø§Ù„Ø®Ø§ØªÙ…Ø©.
Ù„ÙƒÙ„ Ù‚Ø³Ù…:
- "section": Ø§Ù„Ø§Ø³Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
- "summary": Ù…Ù„Ø®Øµ Ø¨Ø³ÙŠØ· Ø¯ÙˆÙ† ØªØ­Ø±ÙŠÙ
- "start_page": Ø¯Ø§Ø¦Ù…Ø§Ù‹ 1
- "content": Ø§Ù„Ù†Øµ Ø§Ù„ÙƒØ§Ù…Ù„ ÙƒÙ…Ø§ Ù‡Ùˆ.

Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø·.
Ø§Ù„Ù†Øµ:
{text[:20000]}
"""
        reply = _llm_json_only(prompt)
        data = _safe_json_loads(reply)
        return data or []


# ============================================================
# ğŸ’¡ Ø§Ù‚ØªØ±Ø§Ø­ Ù…Ø¹Ø§ÙŠÙŠØ± Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ
# ============================================================
def suggest_criteria_from_offers(offers_texts, base_criteria, lang="ar"):
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        st.error("âŒ Ù„Ù… ÙŠØªÙ… Ø¶Ø¨Ø· Ù…ÙØªØ§Ø­ GROQ_API_KEY ÙÙŠ Ù…Ù„Ù .env")
        return []

    client = Groq(api_key=api_key)
    joined = "\n\n---\n\n".join(offers_texts)[:12000]
    seed = ", ".join(base_criteria[:15])

    system = (
        "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ù…Ù†Ø§Ù‚ØµØ§Øª. Ø§Ù‚ØªØ±Ø­ Ù…Ø¹Ø§ÙŠÙŠØ± ØªÙ‚ÙŠÙŠÙ… Ø¥Ø¶Ø§ÙÙŠØ© Ù…Ø®ØªØµØ±Ø© ÙˆÙˆØ§Ø¶Ø­Ø© "
        "ØªØªÙ†Ø§Ø³Ø¨ Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ø±ÙˆØ¶ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø©. Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· "
        "Ø¹Ù„Ù‰ Ø´ÙƒÙ„ Ù‚Ø§Ø¦Ù…Ø© Ù†ØµÙˆØµ Ø¹Ø±Ø¨ÙŠØ© Ù…Ø«Ù„: "
        '["Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¹Ø±Ø¶", "Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙ†ÙÙŠØ°", "Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ù…Ø®Ø§Ø·Ø±", ...]'
    )

    user = (
        f"Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø­Ø§Ù„ÙŠØ©:\n{seed}\n\n"
        f"Ù…Ù‚ØªØ·ÙØ§Øª Ù…Ù† Ø§Ù„Ø¹Ø±ÙˆØ¶:\n{joined}\n\n"
        "Ø§Ù‚ØªØ±Ø­ Ø­ØªÙ‰ 10 Ù…Ø¹Ø§ÙŠÙŠØ± Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©ØŒ Ø¨Ø¯ÙˆÙ† ØªÙƒØ±Ø§Ø± Ø£Ùˆ Ø´Ø±Ø­."
    )

    try:
        resp = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[
                {"role": "system", "content": system},
                {"role": "user", "content": user}
            ],
            temperature=0.25,
            max_tokens=600,
        )
        txt = resp.choices[0].message.content.strip()

        # ğŸ§© Ø¥ØµÙ„Ø§Ø­ Ù†Ø§ØªØ¬ Ø§Ù„Ù†Øµ Ù„Ùˆ Ø±Ø¬Ø¹ ÙƒÙ€ string Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† list
        if txt.startswith("[") and txt.endswith("]"):
            try:
                fixed = json.loads(txt)
                if isinstance(fixed, list):
                    return [s.strip() for s in fixed if s.strip()]
            except:
                pass
        if txt.startswith('"[') or txt.startswith("'["):
            txt = txt.strip('"').strip("'")
            try:
                fixed = json.loads(txt)
                if isinstance(fixed, list):
                    return [s.strip() for s in fixed if s.strip()]
            except:
                pass

        # fallback manual parsing
        lines = [l.strip("â€¢- ").strip() for l in txt.splitlines() if l.strip()]
        return [l for l in lines if l]
    except Exception as e:
        st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„Ù…Ø¹Ø§ÙŠÙŠØ±: {e}")
        return []


# ============================================================
# ğŸ§  ØªØ­Ø³ÙŠÙ† ÙˆØªÙ„Ø®ÙŠØµ Ø§Ù„ÙÙ‚Ø±Ø§Øª (Ø¹Ø±Ø¶ Ù…Ù†Ø³Ù‚ Ø¯Ø§Ø®Ù„ Streamlit)
# ============================================================
def summarize_paragraphs_llm(section_text, model="llama-3.3-70b-versatile"):
    if not section_text.strip():
        return {"clean_text": "", "summaries": []}

    prompt = f"""
Ù‚Ø³Ù… Ø§Ù„Ù†Øµ Ø§Ù„ØªØ§Ù„ÙŠ Ø¥Ù„Ù‰ ÙÙ‚Ø±Ø§Øª Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙÙ‡ÙˆÙ…Ø©ØŒ ÙˆÙ„ÙƒÙ„ ÙÙ‚Ø±Ø© Ø§ÙƒØªØ¨ Ù…Ù„Ø®ØµÙ‹Ø§ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ ÙŠØ´Ø±Ø­ ÙÙƒØ±ØªÙ‡Ø§ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø¨Ø¥ÙŠØ¬Ø§Ø².
Ø£Ø¹Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨ØµÙŠØºØ© JSON ÙÙ‚Ø· ÙƒØ§Ù„ØªØ§Ù„ÙŠ:
[
  {{"paragraph": "Ø§Ù„Ù†Øµ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„Ù„ÙÙ‚Ø±Ù‡", "summary_ar": "Ù…Ù„Ø®Øµ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©"}}
]
Ø§Ù„Ù†Øµ:
{section_text[:15000]}
"""

    summaries = []
    try:
        raw = _llm_json_only(prompt, model=model)
        data = _safe_json_loads(raw)
        if isinstance(data, list):
            summaries = data
        else:
            st.warning("âš ï¸ Ù„Ù… ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„ØµÙ†Ø§Ø¹ÙŠ Ù…Ù† Ø¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙŠÙ‚ JSON ØµØ­ÙŠØ­.")
            summaries = [{"paragraph": section_text, "summary_ar": raw.strip()}]
    except Exception as e:
        st.error(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙ„Ø®ÙŠØµ Ø§Ù„ÙÙ‚Ø±Ø§Øª: {e}")
        summaries = [{"paragraph": section_text, "summary_ar": "Ù„Ù… ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ."}]

    clean_text_out = re.sub(r"\s+", " ", section_text).strip()

    # ğŸ¨ Ø¹Ø±Ø¶ Ù…Ù†Ø³Ù‚
    st.markdown("### âœ¨ Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø°ÙƒÙŠ ")
    for idx, item in enumerate(summaries, start=1):
        st.markdown(
            f"""
            <div style='background:#f8f6ff;border-right:5px solid #5A33A4;
                        padding:14px;border-radius:12px;margin-top:10px;'>
                <b>ğŸ”¹ Ø§Ù„ÙÙ‚Ø±Ø© {idx}:</b><br>
                <span style='color:#333;'>{item['paragraph']}</span>
                <hr style='border:none;border-top:1px dashed #ccc;margin:6px 0;'>
                <b>ğŸ’¡ Ø§Ù„Ù…Ù„Ø®Øµ:</b> <span style='color:#5A33A4;'>{item['summary_ar']}</span>
            </div>
            """,
            unsafe_allow_html=True
        )

    return {"clean_text": clean_text_out, "summaries": summaries}
